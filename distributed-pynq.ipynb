{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar-10 testset classification on Pynq Cluster\n",
    "\n",
    "This notebook uses a convolutional QNN to classify the CIFAR-10 dataset. It uses dask to split the inference task to a cluster of Pynq boards connected to this machine.\n",
    "\n",
    "This notebook modifies https://github.com/Xilinx/BNN-PYNQ/blob/master/notebooks/CNV-QNN_Cifar10_Testset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cifar-10 testset\n",
    "\n",
    "This notebook requires the testset from https://www.cs.toronto.edu/~kriz/cifar.html which contains 10000 images that can be processed by CNV network directly without preprocessing.\n",
    "\n",
    "You can download the cifar-10 set from given url and unzip it to a folder as shown below.\n",
    "This may take a while as the training set is included in the archive as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  162M  100  162M    0     0  6841k      0  0:00:24  0:00:24 --:--:-- 9892k\n"
     ]
    }
   ],
   "source": [
    "# Use the command appropriate to your OS - wget (for linux) or curl (for macOS)\n",
    "\n",
    "# !wget https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
    "!curl -O https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\n",
    "#unzip\n",
    "!tar -xf cifar-10-binary.tar.gz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://192.168.2.1:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://192.168.2.1:8787/status' target='_blank'>http://192.168.2.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://192.168.2.1:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress, get_worker\n",
    "\n",
    "# Insert the scheduler IP below (available after running the \"dask-scheduler\" command)\n",
    "client = Client(\"tcp://192.168.2.1:8786\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_on_worker(data):\n",
    "    print(f\"Received task from scheduler with data {len(data)} bytes\")\n",
    "    from multiprocessing import Process,Queue\n",
    "    from PIL import Image\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "    \n",
    "    def use_overlay(queue, file_path, labels):\n",
    "        import bnn\n",
    "        from pynq import Xlnk        \n",
    "        \n",
    "        hw_classifier = bnn.CnvClassifier(bnn.NETWORK_CNVW2A2,'cifar10',bnn.RUNTIME_HW)\n",
    "        print(\"Classifying.....\")\n",
    "        result_W2A2 = hw_classifier.classify_cifars(file_path)\n",
    "        time_W2A2 = hw_classifier.usecPerImage\n",
    "        print(time_W2A2)\n",
    "\n",
    "        countRight = 0\n",
    "        for idx in range(len(labels)):\n",
    "            if labels[idx] == result_W2A2[idx]:\n",
    "                countRight += 1\n",
    "        accuracyW2A2 = countRight*100/len(labels)\n",
    "        print(\"Accuracy W2A2: \",accuracyW2A2,\"%\")\n",
    "\n",
    "        xlnk = Xlnk()\n",
    "        xlnk.xlnk_reset()\n",
    "        queue.put(result_W2A2)\n",
    "        \n",
    "    \n",
    "    labels = []\n",
    "    i = 0\n",
    "    \n",
    "    #Extract labels to calculate accuracy later\n",
    "    while i<len(data):\n",
    "        labels.append(int.from_bytes(data[i:i+1], byteorder=\"big\"))\n",
    "        i += 3073    #(1 byte of label + 32*32*3 bytes of image)\n",
    "    \n",
    "    # Writing to a file is necessary since this overlay expects a file path present on the Pynq board\n",
    "    file_path = \"input_data.bin\"\n",
    "    with open(file_path, \"wb\") as outfile:\n",
    "        outfile.write(data)\n",
    "    \n",
    "    # We need to run the Pynq overlay in a new forked process since it cannot be run in a non-Main thread\n",
    "    queue = Queue()\n",
    "    p = Process(target=use_overlay, args=(queue,file_path, labels))\n",
    "    p.start()\n",
    "    result = queue.get()\n",
    "    p.join()\n",
    "    t1 = time.time()\n",
    "    print(\"EXECUTION TIME ON THIS WORKER: \", t1 - t0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split image data into 2 chunk(s)\n",
      "Result [array([6, 9, 9, ..., 5, 4, 6], dtype=int32), array([6, 7, 9, ..., 1, 1, 5], dtype=int32)]\n",
      "TOTAL EXECUTION TIME:  22.457534313201904\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "num_of_workers = len(client.scheduler_info()[\"workers\"])\n",
    "data_split = []\n",
    "\n",
    "# Split up the dataset into equal sized chunks based on number of available dask workers\n",
    "with open(\"cifar-10-batches-bin/data_batch_1.bin\", \"rb\") as ifile:    \n",
    "    total = ifile.read()\n",
    "    start = 0\n",
    "    chunk_size = int(len(total)/num_of_workers)\n",
    "    for i in range(num_of_workers):\n",
    "        data_split.append(total[start: start+chunk_size])\n",
    "        start += chunk_size\n",
    "    print(f\"Split image data into {num_of_workers} chunk(s)\")\n",
    "    \n",
    "\n",
    "# Scatter the data to the workers before calling run_on_worker on the workers\n",
    "distributed_data = client.scatter(data_split)\n",
    "futures = client.map(run_on_worker, distributed_data)\n",
    "\n",
    "#Print the output returned by the workers\n",
    "print(\"Result\", client.gather(futures))\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"TOTAL EXECUTION TIME: \", t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
